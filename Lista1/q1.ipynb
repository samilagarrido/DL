{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_vectors = np.array([\n",
    "    [1, -1, -1, -1, -1, -1, -1, -1],\n",
    "    [-1, 1, -1, -1, -1, -1, -1, -1],\n",
    "    [-1, -1, 1, -1, -1, -1, -1, -1],\n",
    "    [-1, -1, -1, 1, -1, -1, -1, -1],\n",
    "    [-1, -1, -1, -1, 1, -1, -1, -1],\n",
    "    [-1, -1, -1, -1, -1, 1, -1, -1],\n",
    "    [-1, -1, -1, -1, -1, -1, 1, -1],\n",
    "    [-1, -1, -1, -1, -1, -1, -1, 1]\n",
    "], dtype=np.float32)\n",
    "\n",
    "data = []\n",
    "\n",
    "n = 10000\n",
    "for i in range(n):\n",
    "    instance = np.array([random.randint(0,1), random.randint(0,1), random.randint(0,1)])\n",
    "    label = [instance[0]*4 + instance[1]*2 + instance[2]]\n",
    "    noise = np.array([random.uniform(-0.1, 0.1), random.uniform(-0.1, 0.1), random.uniform(-0.1, 0.1)])\n",
    "\n",
    "    data.append(np.concatenate([instance + noise, label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7172/341918777.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  train_tensor = torch.tensor(train_data)\n"
     ]
    }
   ],
   "source": [
    "train_data = data[:int(0.7*n)]\n",
    "val_data = data[int(0.7*n):int(0.8*n)]\n",
    "test_data = data[int(0.8*n):]\n",
    "\n",
    "train_tensor = torch.tensor(train_data)\n",
    "val_tensor = torch.tensor(val_data)\n",
    "test_tensor = torch.tensor(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = torch.tensor(train_data)\n",
    "val_tensor = torch.tensor(val_data)\n",
    "test_tensor = torch.tensor(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vertexes(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.data[idx, :3]\n",
    "        y = self.data[idx, 3]\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_dataloader = DataLoader(Vertexes(train_tensor), batch_size=BATCH_SIZE)\n",
    "val_dataloader = DataLoader(Vertexes(val_tensor), batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(Vertexes(test_tensor), batch_size=BATCH_SIZE)\n",
    "\n",
    "device = ( \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\")\n",
    "\n",
    "class Rosemblatt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Rosemblatt, self).__init__()\n",
    "        self.perceptron = nn.Linear(3, 8, dtype=torch.float64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.perceptron(x)\n",
    "        output = F.softmax(x, dim=1, dtype=torch.float64)\n",
    "        return output\n",
    "\n",
    "model = Rosemblatt().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n",
    "     \n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        y = y.type(torch.LongTensor)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() \n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad(): \n",
    "        for X, y in dataloader:\n",
    "            y = y.type(torch.LongTensor)\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 -----------------\n",
      "Accuracy: 12.4%, Avg loss: 2.092606 \n",
      "\n",
      "Epoch 2 -----------------\n",
      "Accuracy: 12.4%, Avg loss: 2.087980 \n",
      "\n",
      "Epoch 3 -----------------\n",
      "Accuracy: 12.4%, Avg loss: 2.083317 \n",
      "\n",
      "Epoch 4 -----------------\n",
      "Accuracy: 13.0%, Avg loss: 2.078627 \n",
      "\n",
      "Epoch 5 -----------------\n",
      "Accuracy: 13.0%, Avg loss: 2.073924 \n",
      "\n",
      "Epoch 6 -----------------\n",
      "Accuracy: 13.0%, Avg loss: 2.069221 \n",
      "\n",
      "Epoch 7 -----------------\n",
      "Accuracy: 13.1%, Avg loss: 2.064528 \n",
      "\n",
      "Epoch 8 -----------------\n",
      "Accuracy: 13.3%, Avg loss: 2.059848 \n",
      "\n",
      "Epoch 9 -----------------\n",
      "Accuracy: 14.1%, Avg loss: 2.055180 \n",
      "\n",
      "Epoch 10 -----------------\n",
      "Accuracy: 14.7%, Avg loss: 2.050516 \n",
      "\n",
      "Epoch 11 -----------------\n",
      "Accuracy: 15.7%, Avg loss: 2.045844 \n",
      "\n",
      "Epoch 12 -----------------\n",
      "Accuracy: 16.6%, Avg loss: 2.041147 \n",
      "\n",
      "Epoch 13 -----------------\n",
      "Accuracy: 18.1%, Avg loss: 2.036407 \n",
      "\n",
      "Epoch 14 -----------------\n",
      "Accuracy: 19.2%, Avg loss: 2.031607 \n",
      "\n",
      "Epoch 15 -----------------\n",
      "Accuracy: 20.4%, Avg loss: 2.026729 \n",
      "\n",
      "Epoch 16 -----------------\n",
      "Accuracy: 24.1%, Avg loss: 2.021759 \n",
      "\n",
      "Epoch 17 -----------------\n",
      "Accuracy: 30.7%, Avg loss: 2.016688 \n",
      "\n",
      "Epoch 18 -----------------\n",
      "Accuracy: 34.8%, Avg loss: 2.011510 \n",
      "\n",
      "Epoch 19 -----------------\n",
      "Accuracy: 36.5%, Avg loss: 2.006226 \n",
      "\n",
      "Epoch 20 -----------------\n",
      "Accuracy: 37.4%, Avg loss: 2.000846 \n",
      "\n",
      "Epoch 21 -----------------\n",
      "Accuracy: 37.8%, Avg loss: 1.995386 \n",
      "\n",
      "Epoch 22 -----------------\n",
      "Accuracy: 37.8%, Avg loss: 1.989870 \n",
      "\n",
      "Epoch 23 -----------------\n",
      "Accuracy: 37.8%, Avg loss: 1.984329 \n",
      "\n",
      "Epoch 24 -----------------\n",
      "Accuracy: 37.8%, Avg loss: 1.978800 \n",
      "\n",
      "Epoch 25 -----------------\n",
      "Accuracy: 37.8%, Avg loss: 1.973314 \n",
      "\n",
      "Epoch 26 -----------------\n",
      "Accuracy: 37.8%, Avg loss: 1.967900 \n",
      "\n",
      "Epoch 27 -----------------\n",
      "Accuracy: 37.8%, Avg loss: 1.962576 \n",
      "\n",
      "Epoch 28 -----------------\n",
      "Accuracy: 37.8%, Avg loss: 1.957347 \n",
      "\n",
      "Epoch 29 -----------------\n",
      "Accuracy: 37.8%, Avg loss: 1.952207 \n",
      "\n",
      "Epoch 30 -----------------\n",
      "Accuracy: 37.8%, Avg loss: 1.947140 \n",
      "\n",
      "Epoch 31 -----------------\n",
      "Accuracy: 37.8%, Avg loss: 1.942124 \n",
      "\n",
      "Epoch 32 -----------------\n",
      "Accuracy: 37.8%, Avg loss: 1.937133 \n",
      "\n",
      "Epoch 33 -----------------\n",
      "Accuracy: 37.8%, Avg loss: 1.932141 \n",
      "\n",
      "Epoch 34 -----------------\n",
      "Accuracy: 37.8%, Avg loss: 1.927122 \n",
      "\n",
      "Epoch 35 -----------------\n",
      "Accuracy: 37.8%, Avg loss: 1.922051 \n",
      "\n",
      "Epoch 36 -----------------\n",
      "Accuracy: 37.8%, Avg loss: 1.916909 \n",
      "\n",
      "Epoch 37 -----------------\n",
      "Accuracy: 37.8%, Avg loss: 1.911678 \n",
      "\n",
      "Epoch 38 -----------------\n",
      "Accuracy: 38.2%, Avg loss: 1.906343 \n",
      "\n",
      "Epoch 39 -----------------\n",
      "Accuracy: 40.1%, Avg loss: 1.900897 \n",
      "\n",
      "Epoch 40 -----------------\n",
      "Accuracy: 43.0%, Avg loss: 1.895334 \n",
      "\n",
      "Epoch 41 -----------------\n",
      "Accuracy: 45.1%, Avg loss: 1.889655 \n",
      "\n",
      "Epoch 42 -----------------\n",
      "Accuracy: 47.6%, Avg loss: 1.883864 \n",
      "\n",
      "Epoch 43 -----------------\n",
      "Accuracy: 51.9%, Avg loss: 1.877972 \n",
      "\n",
      "Epoch 44 -----------------\n",
      "Accuracy: 57.2%, Avg loss: 1.871994 \n",
      "\n",
      "Epoch 45 -----------------\n",
      "Accuracy: 60.6%, Avg loss: 1.865946 \n",
      "\n",
      "Epoch 46 -----------------\n",
      "Accuracy: 62.7%, Avg loss: 1.859851 \n",
      "\n",
      "Epoch 47 -----------------\n",
      "Accuracy: 63.9%, Avg loss: 1.853729 \n",
      "\n",
      "Epoch 48 -----------------\n",
      "Accuracy: 63.9%, Avg loss: 1.847604 \n",
      "\n",
      "Epoch 49 -----------------\n",
      "Accuracy: 63.9%, Avg loss: 1.841499 \n",
      "\n",
      "Epoch 50 -----------------\n",
      "Accuracy: 63.9%, Avg loss: 1.835435 \n",
      "\n",
      "Epoch 51 -----------------\n",
      "Accuracy: 63.9%, Avg loss: 1.829435 \n",
      "\n",
      "Epoch 52 -----------------\n",
      "Accuracy: 63.9%, Avg loss: 1.823517 \n",
      "\n",
      "Epoch 53 -----------------\n",
      "Accuracy: 63.9%, Avg loss: 1.817695 \n",
      "\n",
      "Epoch 54 -----------------\n",
      "Accuracy: 63.9%, Avg loss: 1.811982 \n",
      "\n",
      "Epoch 55 -----------------\n",
      "Accuracy: 63.9%, Avg loss: 1.806384 \n",
      "\n",
      "Epoch 56 -----------------\n",
      "Accuracy: 63.9%, Avg loss: 1.800906 \n",
      "\n",
      "Epoch 57 -----------------\n",
      "Accuracy: 63.9%, Avg loss: 1.795546 \n",
      "\n",
      "Epoch 58 -----------------\n",
      "Accuracy: 63.9%, Avg loss: 1.790299 \n",
      "\n",
      "Epoch 59 -----------------\n",
      "Accuracy: 63.9%, Avg loss: 1.785160 \n",
      "\n",
      "Epoch 60 -----------------\n",
      "Accuracy: 63.9%, Avg loss: 1.780116 \n",
      "\n",
      "Epoch 61 -----------------\n",
      "Accuracy: 63.9%, Avg loss: 1.775159 \n",
      "\n",
      "Epoch 62 -----------------\n",
      "Accuracy: 63.9%, Avg loss: 1.770274 \n",
      "\n",
      "Epoch 63 -----------------\n",
      "Accuracy: 63.9%, Avg loss: 1.765448 \n",
      "\n",
      "Epoch 64 -----------------\n",
      "Accuracy: 63.9%, Avg loss: 1.760665 \n",
      "\n",
      "Epoch 65 -----------------\n",
      "Accuracy: 63.9%, Avg loss: 1.755910 \n",
      "\n",
      "Epoch 66 -----------------\n",
      "Accuracy: 63.9%, Avg loss: 1.751166 \n",
      "\n",
      "Epoch 67 -----------------\n",
      "Accuracy: 63.9%, Avg loss: 1.746418 \n",
      "\n",
      "Epoch 68 -----------------\n",
      "Accuracy: 63.9%, Avg loss: 1.741648 \n",
      "\n",
      "Epoch 69 -----------------\n",
      "Accuracy: 63.9%, Avg loss: 1.736843 \n",
      "\n",
      "Epoch 70 -----------------\n",
      "Accuracy: 64.0%, Avg loss: 1.731990 \n",
      "\n",
      "Epoch 71 -----------------\n",
      "Accuracy: 64.8%, Avg loss: 1.727080 \n",
      "\n",
      "Epoch 72 -----------------\n",
      "Accuracy: 66.9%, Avg loss: 1.722111 \n",
      "\n",
      "Epoch 73 -----------------\n",
      "Accuracy: 69.4%, Avg loss: 1.717085 \n",
      "\n",
      "Epoch 74 -----------------\n",
      "Accuracy: 72.0%, Avg loss: 1.712013 \n",
      "\n",
      "Epoch 75 -----------------\n",
      "Accuracy: 74.2%, Avg loss: 1.706915 \n",
      "\n",
      "Epoch 76 -----------------\n",
      "Accuracy: 75.6%, Avg loss: 1.701815 \n",
      "\n",
      "Epoch 77 -----------------\n",
      "Accuracy: 76.0%, Avg loss: 1.696746 \n",
      "\n",
      "Epoch 78 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.691738 \n",
      "\n",
      "Epoch 79 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.686820 \n",
      "\n",
      "Epoch 80 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.682018 \n",
      "\n",
      "Epoch 81 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.677348 \n",
      "\n",
      "Epoch 82 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.672822 \n",
      "\n",
      "Epoch 83 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.668443 \n",
      "\n",
      "Epoch 84 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.664212 \n",
      "\n",
      "Epoch 85 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.660123 \n",
      "\n",
      "Epoch 86 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.656172 \n",
      "\n",
      "Epoch 87 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.652349 \n",
      "\n",
      "Epoch 88 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.648648 \n",
      "\n",
      "Epoch 89 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.645061 \n",
      "\n",
      "Epoch 90 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.641581 \n",
      "\n",
      "Epoch 91 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.638201 \n",
      "\n",
      "Epoch 92 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.634913 \n",
      "\n",
      "Epoch 93 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.631711 \n",
      "\n",
      "Epoch 94 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.628589 \n",
      "\n",
      "Epoch 95 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.625542 \n",
      "\n",
      "Epoch 96 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.622563 \n",
      "\n",
      "Epoch 97 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.619647 \n",
      "\n",
      "Epoch 98 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.616789 \n",
      "\n",
      "Epoch 99 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.613983 \n",
      "\n",
      "Epoch 100 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.611225 \n",
      "\n",
      "Epoch 101 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.608509 \n",
      "\n",
      "Epoch 102 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.605830 \n",
      "\n",
      "Epoch 103 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.603185 \n",
      "\n",
      "Epoch 104 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.600567 \n",
      "\n",
      "Epoch 105 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.597974 \n",
      "\n",
      "Epoch 106 -----------------\n",
      "Accuracy: 76.1%, Avg loss: 1.595400 \n",
      "\n",
      "Epoch 107 -----------------\n",
      "Accuracy: 76.4%, Avg loss: 1.592841 \n",
      "\n",
      "Epoch 108 -----------------\n",
      "Accuracy: 76.8%, Avg loss: 1.590293 \n",
      "\n",
      "Epoch 109 -----------------\n",
      "Accuracy: 76.8%, Avg loss: 1.587752 \n",
      "\n",
      "Epoch 110 -----------------\n",
      "Accuracy: 78.0%, Avg loss: 1.585215 \n",
      "\n",
      "Epoch 111 -----------------\n",
      "Accuracy: 80.1%, Avg loss: 1.582677 \n",
      "\n",
      "Epoch 112 -----------------\n",
      "Accuracy: 82.1%, Avg loss: 1.580137 \n",
      "\n",
      "Epoch 113 -----------------\n",
      "Accuracy: 83.4%, Avg loss: 1.577591 \n",
      "\n",
      "Epoch 114 -----------------\n",
      "Accuracy: 85.3%, Avg loss: 1.575036 \n",
      "\n",
      "Epoch 115 -----------------\n",
      "Accuracy: 86.3%, Avg loss: 1.572472 \n",
      "\n",
      "Epoch 116 -----------------\n",
      "Accuracy: 87.5%, Avg loss: 1.569896 \n",
      "\n",
      "Epoch 117 -----------------\n",
      "Accuracy: 88.1%, Avg loss: 1.567308 \n",
      "\n",
      "Epoch 118 -----------------\n",
      "Accuracy: 88.8%, Avg loss: 1.564708 \n",
      "\n",
      "Epoch 119 -----------------\n",
      "Accuracy: 88.9%, Avg loss: 1.562095 \n",
      "\n",
      "Epoch 120 -----------------\n",
      "Accuracy: 89.3%, Avg loss: 1.559472 \n",
      "\n",
      "Epoch 121 -----------------\n",
      "Accuracy: 89.4%, Avg loss: 1.556841 \n",
      "\n",
      "Epoch 122 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.554203 \n",
      "\n",
      "Epoch 123 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.551561 \n",
      "\n",
      "Epoch 124 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.548920 \n",
      "\n",
      "Epoch 125 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.546284 \n",
      "\n",
      "Epoch 126 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.543655 \n",
      "\n",
      "Epoch 127 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.541039 \n",
      "\n",
      "Epoch 128 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.538439 \n",
      "\n",
      "Epoch 129 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.535858 \n",
      "\n",
      "Epoch 130 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.533301 \n",
      "\n",
      "Epoch 131 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.530770 \n",
      "\n",
      "Epoch 132 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.528267 \n",
      "\n",
      "Epoch 133 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.525792 \n",
      "\n",
      "Epoch 134 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.523347 \n",
      "\n",
      "Epoch 135 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.520930 \n",
      "\n",
      "Epoch 136 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.518541 \n",
      "\n",
      "Epoch 137 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.516176 \n",
      "\n",
      "Epoch 138 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.513831 \n",
      "\n",
      "Epoch 139 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.511503 \n",
      "\n",
      "Epoch 140 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.509185 \n",
      "\n",
      "Epoch 141 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.506868 \n",
      "\n",
      "Epoch 142 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.504546 \n",
      "\n",
      "Epoch 143 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.502205 \n",
      "\n",
      "Epoch 144 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.499834 \n",
      "\n",
      "Epoch 145 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.497418 \n",
      "\n",
      "Epoch 146 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.494940 \n",
      "\n",
      "Epoch 147 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.492380 \n",
      "\n",
      "Epoch 148 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.489718 \n",
      "\n",
      "Epoch 149 -----------------\n",
      "Accuracy: 89.5%, Avg loss: 1.486936 \n",
      "\n",
      "Epoch 150 -----------------\n",
      "Accuracy: 89.7%, Avg loss: 1.484015 \n",
      "\n",
      "Epoch 151 -----------------\n",
      "Accuracy: 92.4%, Avg loss: 1.480950 \n",
      "\n",
      "Epoch 152 -----------------\n",
      "Accuracy: 96.1%, Avg loss: 1.477745 \n",
      "\n",
      "Epoch 153 -----------------\n",
      "Accuracy: 99.6%, Avg loss: 1.474427 \n",
      "\n",
      "Epoch 154 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.471043 \n",
      "\n",
      "Epoch 155 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.467655 \n",
      "\n",
      "Epoch 156 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.464331 \n",
      "\n",
      "Epoch 157 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.461128 \n",
      "\n",
      "Epoch 158 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.458079 \n",
      "\n",
      "Epoch 159 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.455194 \n",
      "\n",
      "Epoch 160 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.452461 \n",
      "\n",
      "Epoch 161 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.449861 \n",
      "\n",
      "Epoch 162 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.447370 \n",
      "\n",
      "Epoch 163 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.444970 \n",
      "\n",
      "Epoch 164 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.442644 \n",
      "\n",
      "Epoch 165 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.440383 \n",
      "\n",
      "Epoch 166 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.438180 \n",
      "\n",
      "Epoch 167 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.436028 \n",
      "\n",
      "Epoch 168 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.433927 \n",
      "\n",
      "Epoch 169 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.431873 \n",
      "\n",
      "Epoch 170 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.429866 \n",
      "\n",
      "Epoch 171 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.427903 \n",
      "\n",
      "Epoch 172 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.425985 \n",
      "\n",
      "Epoch 173 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.424109 \n",
      "\n",
      "Epoch 174 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.422276 \n",
      "\n",
      "Epoch 175 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.420483 \n",
      "\n",
      "Epoch 176 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.418730 \n",
      "\n",
      "Epoch 177 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.417015 \n",
      "\n",
      "Epoch 178 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.415339 \n",
      "\n",
      "Epoch 179 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.413699 \n",
      "\n",
      "Epoch 180 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.412095 \n",
      "\n",
      "Epoch 181 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.410526 \n",
      "\n",
      "Epoch 182 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.408990 \n",
      "\n",
      "Epoch 183 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.407487 \n",
      "\n",
      "Epoch 184 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.406016 \n",
      "\n",
      "Epoch 185 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.404576 \n",
      "\n",
      "Epoch 186 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.403166 \n",
      "\n",
      "Epoch 187 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.401785 \n",
      "\n",
      "Epoch 188 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.400432 \n",
      "\n",
      "Epoch 189 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.399107 \n",
      "\n",
      "Epoch 190 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.397809 \n",
      "\n",
      "Epoch 191 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.396536 \n",
      "\n",
      "Epoch 192 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.395289 \n",
      "\n",
      "Epoch 193 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.394067 \n",
      "\n",
      "Epoch 194 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.392868 \n",
      "\n",
      "Epoch 195 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.391692 \n",
      "\n",
      "Epoch 196 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.390539 \n",
      "\n",
      "Epoch 197 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.389408 \n",
      "\n",
      "Epoch 198 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.388298 \n",
      "\n",
      "Epoch 199 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.387209 \n",
      "\n",
      "Epoch 200 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.386140 \n",
      "\n",
      "Epoch 201 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.385091 \n",
      "\n",
      "Epoch 202 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.384061 \n",
      "\n",
      "Epoch 203 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.383050 \n",
      "\n",
      "Epoch 204 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.382056 \n",
      "\n",
      "Epoch 205 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.381081 \n",
      "\n",
      "Epoch 206 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.380122 \n",
      "\n",
      "Epoch 207 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.379181 \n",
      "\n",
      "Epoch 208 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.378255 \n",
      "\n",
      "Epoch 209 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.377346 \n",
      "\n",
      "Epoch 210 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.376452 \n",
      "\n",
      "Epoch 211 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.375574 \n",
      "\n",
      "Epoch 212 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.374710 \n",
      "\n",
      "Epoch 213 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.373861 \n",
      "\n",
      "Epoch 214 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.373025 \n",
      "\n",
      "Epoch 215 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.372203 \n",
      "\n",
      "Epoch 216 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.371395 \n",
      "\n",
      "Epoch 217 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.370600 \n",
      "\n",
      "Epoch 218 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.369818 \n",
      "\n",
      "Epoch 219 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.369048 \n",
      "\n",
      "Epoch 220 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.368290 \n",
      "\n",
      "Epoch 221 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.367544 \n",
      "\n",
      "Epoch 222 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.366810 \n",
      "\n",
      "Epoch 223 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.366087 \n",
      "\n",
      "Epoch 224 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.365375 \n",
      "\n",
      "Epoch 225 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.364674 \n",
      "\n",
      "Epoch 226 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.363983 \n",
      "\n",
      "Epoch 227 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.363303 \n",
      "\n",
      "Epoch 228 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.362633 \n",
      "\n",
      "Epoch 229 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.361973 \n",
      "\n",
      "Epoch 230 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.361323 \n",
      "\n",
      "Epoch 231 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.360682 \n",
      "\n",
      "Epoch 232 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.360050 \n",
      "\n",
      "Epoch 233 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.359428 \n",
      "\n",
      "Epoch 234 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.358814 \n",
      "\n",
      "Epoch 235 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.358209 \n",
      "\n",
      "Epoch 236 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.357613 \n",
      "\n",
      "Epoch 237 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.357025 \n",
      "\n",
      "Epoch 238 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.356445 \n",
      "\n",
      "Epoch 239 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.355873 \n",
      "\n",
      "Epoch 240 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.355309 \n",
      "\n",
      "Epoch 241 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.354753 \n",
      "\n",
      "Epoch 242 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.354204 \n",
      "\n",
      "Epoch 243 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.353662 \n",
      "\n",
      "Epoch 244 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.353128 \n",
      "\n",
      "Epoch 245 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.352601 \n",
      "\n",
      "Epoch 246 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.352080 \n",
      "\n",
      "Epoch 247 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.351567 \n",
      "\n",
      "Epoch 248 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.351060 \n",
      "\n",
      "Epoch 249 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.350560 \n",
      "\n",
      "Epoch 250 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.350066 \n",
      "\n",
      "Epoch 251 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.349579 \n",
      "\n",
      "Epoch 252 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.349097 \n",
      "\n",
      "Epoch 253 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.348622 \n",
      "\n",
      "Epoch 254 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.348153 \n",
      "\n",
      "Epoch 255 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.347689 \n",
      "\n",
      "Epoch 256 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.347231 \n",
      "\n",
      "Epoch 257 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.346779 \n",
      "\n",
      "Epoch 258 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.346333 \n",
      "\n",
      "Epoch 259 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.345891 \n",
      "\n",
      "Epoch 260 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.345456 \n",
      "\n",
      "Epoch 261 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.345025 \n",
      "\n",
      "Epoch 262 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.344600 \n",
      "\n",
      "Epoch 263 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.344179 \n",
      "\n",
      "Epoch 264 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.343764 \n",
      "\n",
      "Epoch 265 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.343353 \n",
      "\n",
      "Epoch 266 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.342947 \n",
      "\n",
      "Epoch 267 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.342546 \n",
      "\n",
      "Epoch 268 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.342150 \n",
      "\n",
      "Epoch 269 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.341758 \n",
      "\n",
      "Epoch 270 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.341370 \n",
      "\n",
      "Epoch 271 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.340987 \n",
      "\n",
      "Epoch 272 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.340608 \n",
      "\n",
      "Epoch 273 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.340234 \n",
      "\n",
      "Epoch 274 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.339863 \n",
      "\n",
      "Epoch 275 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.339497 \n",
      "\n",
      "Epoch 276 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.339135 \n",
      "\n",
      "Epoch 277 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.338777 \n",
      "\n",
      "Epoch 278 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.338422 \n",
      "\n",
      "Epoch 279 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.338072 \n",
      "\n",
      "Epoch 280 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.337725 \n",
      "\n",
      "Epoch 281 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.337382 \n",
      "\n",
      "Epoch 282 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.337043 \n",
      "\n",
      "Epoch 283 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.336707 \n",
      "\n",
      "Epoch 284 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.336375 \n",
      "\n",
      "Epoch 285 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.336046 \n",
      "\n",
      "Epoch 286 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.335721 \n",
      "\n",
      "Epoch 287 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.335399 \n",
      "\n",
      "Epoch 288 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.335080 \n",
      "\n",
      "Epoch 289 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.334765 \n",
      "\n",
      "Epoch 290 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.334453 \n",
      "\n",
      "Epoch 291 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.334144 \n",
      "\n",
      "Epoch 292 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.333838 \n",
      "\n",
      "Epoch 293 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.333536 \n",
      "\n",
      "Epoch 294 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.333236 \n",
      "\n",
      "Epoch 295 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.332939 \n",
      "\n",
      "Epoch 296 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.332646 \n",
      "\n",
      "Epoch 297 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.332355 \n",
      "\n",
      "Epoch 298 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.332067 \n",
      "\n",
      "Epoch 299 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.331782 \n",
      "\n",
      "Epoch 300 -----------------\n",
      "Accuracy: 100.0%, Avg loss: 1.331499 \n",
      "\n",
      "-----------------Done-----------------\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 300\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t+1} -----------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(val_dataloader, model, loss_fn)\n",
    "print(\"-----------------Done-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0%, Avg loss: 1.331580 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(test_dataloader, model, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
